# üöÄ IH Ingestion Generator

Ferramenta em Go para gerar automaticamente:

- Kafka Connect **source connectors** (Debezium SQL Server)
- Kafka Connect **sink connectors** (Snowflake)
- **Jobs** de cria√ß√£o de tabelas no Snowflake (tabela `_INGEST`, tabela final e STAGE)

A partir de:

- Metadados reais das tabelas no SQL Server (INFORMATION_SCHEMA)
- Um arquivo declarativo `ingestion.yaml`
- Vari√°veis de ambiente (`.env` em dev, Secrets em prod)

## üß≠ Diagrama de arquitetura

```mermaid
flowchart TD
    SQL[SQL Server
    Tabelas e metadados] -->|INFORMATION_SCHEMA| CLI[Ingestion CLI]
    INGESTION[Arquivo declarativo
    ingestion.yaml] --> CLI
    ENV[Vari√°veis de ambiente
    (.env ou secrets)] --> CLI
    CLI -->|Gera√ß√£o de artefatos| GEN[Conectores e jobs]
    GEN --> SRC[Kafka Connect
    Debezium SQL Server (source)]
    SRC --> KAFKA[(Kafka t√≥picos)]
    KAFKA --> SNK[Kafka Connect
    Snowflake (sink)]
    SNK --> STAGE[Snowflake
    Tabela STAGE]
    STAGE --> INGEST[_INGEST]
    INGEST --> FINAL[Tabela final]
```

---

## üîß Como rodar em modo single (tabela √∫nica)

```bash
cp .env.example .env   # ajustar valores

go run ./cmd/ingestion-cli \
  -schema dbo \
  -table Clientes \
  -out ./out

```

## üóíÔ∏è Checklist de configura√ß√£o

- **SQL Server:** usu√°rio com permiss√£o de leitura em `INFORMATION_SCHEMA` para coletar metadados das tabelas.
- **Kafka Connect Debezium:** cluster acess√≠vel a partir do ambiente onde o CLI ser√° executado; `BOOTSTRAP_SERVERS` e `CONNECT_URL` definidos.
- **Snowflake:** warehouse, database e schema j√° criados; role com permiss√£o de cria√ß√£o de tabelas e stage.
- **Arquivos de ambiente:** copie o `.env.example` e preencha vari√°veis obrigat√≥rias (conex√£o SQL Server, Kafka, Snowflake, t√≥pico). Em produ√ß√£o, use secrets/vari√°veis do runner em vez de `.env`.
- **Go 1.21+** instalado para rodar o CLI localmente.

## üßæ Exemplo m√≠nimo de `ingestion.yaml`

```yaml
# Define um pipeline simples para uma tabela
pipeline:
  name: clientes_pipeline
  source:
    schema: dbo
    table: Clientes
  sink:
    snowflake:
      database: ANALYTICS
      schema: LANDING
      table: CLIENTES

# Opcional: m√∫ltiplas tabelas podem ser listadas
additional_tables:
  - schema: dbo
    table: Pedidos
  - schema: dbo
    table: Produtos
```

## üß∞ Rodando em lote (m√∫ltiplas tabelas)

```bash
go run ./cmd/ingestion-cli \
  -config ./ingestion.yaml \
  -out ./out
```

O CLI l√™ o `ingestion.yaml` e gera conectores para cada tabela listada, al√©m dos artefatos de stage/final. Os nomes de t√≥picos e tabelas de destino seguem as configura√ß√µes do arquivo.

## üóÇÔ∏è Artefatos gerados

- **Conector Debezium (source)**: JSON para cria√ß√£o no Kafka Connect, configurando captura de mudan√ßas no SQL Server.
- **Conector Snowflake (sink)**: JSON para ingest√£o dos t√≥picos no stage Snowflake.
- **Jobs de tabelas**: scripts para criar `_INGEST`, tabela final e STAGE com colunas alinhadas ao schema real.
- **Log de execu√ß√£o**: arquivos informando tabelas encontradas, colunas ignoradas e valida√ß√µes feitas durante o parsing do `ingestion.yaml`.

## üõ†Ô∏è Dicas e troubleshooting

- Certifique-se de que a porta do SQL Server esteja acess√≠vel e que a vari√°vel `SQLSERVER_PORT` corresponda ao ambiente.
- Caso o Debezium n√£o veja novas mudan√ßas, verifique permiss√µes de CDC na tabela origem.
- Erros ao criar tabelas no Snowflake geralmente est√£o ligados a role/warehouse incorreto; revise `SNOWFLAKE_ROLE` e `SNOWFLAKE_WAREHOUSE`.
- Para depurar, execute com `LOG_LEVEL=debug` no `.env` e inspecione os arquivos de sa√≠da em `./out`.